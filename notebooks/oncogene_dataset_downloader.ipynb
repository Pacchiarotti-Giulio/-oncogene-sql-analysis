{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a302ce8",
   "metadata": {},
   "source": [
    "# Oncogene Dataset Downloader\n",
    "This notebook automates the process of downloading, processing, and exporting biomedical datasets for oncogene analysis using public APIs and bioinformatics libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "# !pip install bioservices\n",
    "# !pip install elementpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from io import StringIO\n",
    "from Bio.ExPASy import ScanProsite, Prosite\n",
    "from Bio import SeqIO\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir(\".\")\n",
    "\n",
    "# Load cancer and healthy drivers datasets\n",
    "cancer_drivers = pd.read_csv(\"datasets/NCG_cancerdrivers_annotation_supporting_evidence.tsv\", sep=\"\\t\")\n",
    "healthy_drivers = pd.read_csv(\"datasets/NCG_healthydrivers_annotation_supporting_evidence.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Add driver type\n",
    "cancer_drivers[\"driver_type\"] = \"cancer\"\n",
    "healthy_drivers[\"driver_type\"] = \"healthy\"\n",
    "\n",
    "# Drop problematic gene\n",
    "cancer_drivers.drop(cancer_drivers[cancer_drivers[\"symbol\"] == \"WAS\"].index, inplace=True)\n",
    "\n",
    "# Select and merge columns\n",
    "healthy_drivers = healthy_drivers[['entrez', 'symbol', 'pubmed_id', 'type', 'organ_system']]\n",
    "drivers = pd.concat([cancer_drivers, healthy_drivers])\n",
    "\n",
    "# Save datasets\n",
    "cancer_drivers.to_csv(\"datasets/cancer_drivers.csv\", index=False)\n",
    "healthy_drivers.to_csv(\"datasets/healthy_drivers.csv\", index=False)\n",
    "drivers.to_csv(\"datasets/drivers.csv\", index=False)\n",
    "\n",
    "# Display unique genes\n",
    "gene_symbols = drivers[\"symbol\"].unique()\n",
    "print(f\"Number of unique driver genes: {len(gene_symbols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a469a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioservices import UniProt\n",
    "\n",
    "u = UniProt()\n",
    "uniprot_info = pd.DataFrame()\n",
    "\n",
    "for i, gene in enumerate(gene_symbols):\n",
    "    info = u.search(query=f\"gene_exact:{gene.strip()}+AND+organism_id:9606+AND+reviewed:true\", frmt=\"tsv\")\n",
    "    df_info = pd.read_csv(StringIO(info), sep=\"\\t\")\n",
    "    df_info[\"Gene Name Reference\"] = df_info[\"Gene Names\"].str.split().str[0]\n",
    "    uniprot_info = pd.concat([uniprot_info, df_info])\n",
    "    print(f\"Gene {i}: {gene}\")\n",
    "\n",
    "uniprot_info.to_csv(\"uniprot_info.csv\", index=False)\n",
    "\n",
    "# Get detailed info for each entry\n",
    "AC_uniprot = list(uniprot_info[\"Entry\"])\n",
    "uniprot_info_2 = pd.DataFrame()\n",
    "\n",
    "for i, ac in enumerate(AC_uniprot):\n",
    "    df = u.get_df(ac)\n",
    "    uniprot_info_2 = pd.concat([uniprot_info_2, df])\n",
    "    print(f\"Accession {i}: {ac}\")\n",
    "\n",
    "uniprot_info_2.to_csv(\"datasets/uniprot_info_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop problematic sequences by index\n",
    "uniprot_info_2.drop([1961, 2893], inplace=True)\n",
    "\n",
    "# ScanProsite and write motif info\n",
    "with open(\"datasets/prosite_protein_info.csv\", 'w', newline='') as file_csv:\n",
    "    writer = csv.writer(file_csv)\n",
    "    for i, sequence in enumerate(uniprot_info_2[\"Sequence\"]):\n",
    "        try:\n",
    "            result = ScanProsite.scan(seq=sequence).read().decode('utf-8')\n",
    "            root = ET.fromstring(result)\n",
    "            uniprot_ac = uniprot_info_2.iloc[i][\"Entry\"]\n",
    "            for match in root.findall('.//{urn:expasy:scanprosite}match'):\n",
    "                signature_ac = match.find('{urn:expasy:scanprosite}signature_ac').text\n",
    "                start = match.find('{urn:expasy:scanprosite}start').text\n",
    "                stop = match.find('{urn:expasy:scanprosite}stop').text\n",
    "                sequence_ac = match.find('{urn:expasy:scanprosite}sequence_ac').text\n",
    "                writer.writerow([uniprot_ac, signature_ac, start, stop, sequence_ac])\n",
    "        except Exception as e:\n",
    "            print(f\"Error on sequence {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9463a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prosite_protein_info = pd.read_csv(\"datasets/prosite_protein_info.csv\", names=[\"uniprot_ac\", \"signature_ac\", \"start\", \"stop\", \"sequence_ac\"])\n",
    "prosite_protein_info.to_csv(\"datasets/prosite_protein_info_total.csv\", index=False)\n",
    "\n",
    "# Parse prosite.dat for motif information\n",
    "AC_prosite = prosite_protein_info[\"signature_ac\"].unique()\n",
    "prosite_info = []\n",
    "\n",
    "with open(\"prosite.dat\") as handle:\n",
    "    for i, ac in enumerate(AC_prosite):\n",
    "        print(f\"Motif {i}: {ac}\")\n",
    "        handle.seek(0)\n",
    "        records = Prosite.parse(handle)\n",
    "        for record in records:\n",
    "            if record.accession == ac:\n",
    "                prosite_info.append([record.accession, record.name, record.description, record.pattern])\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"datasets/prosite_info.csv\", 'w', newline='') as file_csv:\n",
    "    writer = csv.writer(file_csv)\n",
    "    writer.writerows(prosite_info)\n",
    "\n",
    "pd.read_csv(\"datasets/prosite_info.csv\", names=[\"accession\", \"name\", \"description\", \"pattern\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to download TSV and convert to CSV\n",
    "def download_signor_data(url, protein_list, params_base):\n",
    "    for protein in protein_list:\n",
    "        params = {**params_base, \"proteins\": protein}\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                df_tsv = pd.read_csv(StringIO(response.text), sep='\\t')\n",
    "                df_tsv.to_csv(\"datasets/signor_info.csv\", mode='a', index=False, header=False)\n",
    "                print(f\"Downloaded data for {protein}\")\n",
    "            else:\n",
    "                print(f\"Request error for {protein}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {protein}: {e}\")\n",
    "\n",
    "# Define API parameters\n",
    "url_api = \"https://signor.uniroma2.it/CancerGeneNet/getData.php\"\n",
    "params_base = {\"type\": \"shortestPath\", \"phenotype\": \"\", \"output\": \"summary\"}\n",
    "\n",
    "# Run download\n",
    "download_signor_data(url_api, uniprot_info_2['Gene Names (primary)'], params_base)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
